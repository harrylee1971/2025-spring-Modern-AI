{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae05486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train range: 2016-05-11 ~ 2023-05-10 (exclusive)\n",
      "Test  range: 2023-05-10 ~ 2024-05-09\n",
      "Save model to: c:\\Users\\L-JC (Harry)\\Downloads\\ppo\\outputs\\lstm_dueling_double_dqn_best.pth\n",
      "Ep    1/200 | Reward  1126.70 | PV  119310.97 | Loss   0.9219 | eps 0.050 | Cnt(H/B/S)=[156, 454, 1129]\n",
      "Recent actions: ['Hold', 'Buy 542 @ 55.74', 'Hold', 'Buy 268 @ 56.17', 'Hold', 'Sell 946 @ 54.88', 'Hold', 'Buy 605 @ 55.29']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 683\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    678\u001b[39m     \u001b[38;5;66;03m# 你可以改 ticker：\u001b[39;00m\n\u001b[32m    679\u001b[39m     \u001b[38;5;66;03m# - 若你真的是要「0050」：yfinance 通常是 \"0050.TW\"\u001b[39;00m\n\u001b[32m    680\u001b[39m     \u001b[38;5;66;03m# - 你原碼是 \"SPMO\"：我保留但你可自行改\u001b[39;00m\n\u001b[32m    681\u001b[39m     ticker = \u001b[33m\"\u001b[39m\u001b[33mSPMO\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# 例如改成 \"0050.TW\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m     best_model_path, test_csv_path = \u001b[43mtrain_and_test_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_years\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_years\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitial_balance\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m     predict_today_action(\n\u001b[32m    694\u001b[39m         model_path=best_model_path,\n\u001b[32m    695\u001b[39m         ticker=ticker,\n\u001b[32m   (...)\u001b[39m\u001b[32m    698\u001b[39m         lookback_days=\u001b[32m180\u001b[39m,\n\u001b[32m    699\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 543\u001b[39m, in \u001b[36mtrain_and_test_dqn\u001b[39m\u001b[34m(ticker, end_date, train_years, test_years, window_size, initial_balance, episodes)\u001b[39m\n\u001b[32m    540\u001b[39m next_state, reward, done, info = train_env.step(action)\n\u001b[32m    541\u001b[39m agent.remember(state, action, reward, next_state, done)\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m loss = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    545\u001b[39m     losses.append(loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 438\u001b[39m, in \u001b[36mDQNAgent.replay\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# Double DQN：\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# online 網路選下一步 action，target 網路估值\u001b[39;00m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     next_actions = torch.argmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_extra_states\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m    439\u001b[39m     next_q = \u001b[38;5;28mself\u001b[39m.target_model(next_sequences, next_extra_states).gather(\u001b[32m1\u001b[39m, next_actions.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    440\u001b[39m     target_q = rewards + (\u001b[32m1.0\u001b[39m - dones) * \u001b[38;5;28mself\u001b[39m.gamma * next_q\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 345\u001b[39m, in \u001b[36mLSTM_DuelingDQN.forward\u001b[39m\u001b[34m(self, sequence, extra_state)\u001b[39m\n\u001b[32m    342\u001b[39m lstm_feat = lstm_out[:, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# (B, H)\u001b[39;00m\n\u001b[32m    344\u001b[39m combined = torch.cat([lstm_feat, extra_state], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, H+2)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m v = \u001b[38;5;28mself\u001b[39m.value(x)                 \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[32m    348\u001b[39m a = \u001b[38;5;28mself\u001b[39m.advantage(x)             \u001b[38;5;66;03m# (B, A)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:1422\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "LSTM + Dueling Double DQN 股票交易（低買高賣導向）- 修正版（完整可直接跑）\n",
    "\n",
    "你原始版本的主要問題我已一起修掉：\n",
    "1) reset() 把 balance 固定成 4500，和 initial_balance/終止報酬基準 100000 不一致 → 改為一致\n",
    "2) Sell 邏輯：原本算 sold_shares = shares/2 但實際賣出卻把全部賣掉 → 改成真的「賣一半」\n",
    "3) reward 方向：原本買在 >MA 反而鼓勵追高 → 改成鼓勵「價格低於 MA + RSI 偏低」時買；「價格高於 MA + RSI 偏高」時賣\n",
    "4) epsilon 探索：原本 epsilon 分支仍用 model 計算 softmax → 改成標準：epsilon 隨機、否則 greedy（另保留 temperature 做 Boltzmann 可選）\n",
    "5) DQN 更新：改成 Double DQN（online 選 action、target 評估）\n",
    "6) 尺度/資料洩漏：state 的正規化改用「視窗內」與當前參考值（避免用全期間 max）\n",
    "7) 輸出路徑：移除硬編碼桌面路徑，改成自動存到當前資料夾 ./outputs/\n",
    "8) 其他：加入 device、梯度裁剪、訓練過程的 checkpoint / best model、較完整的紀錄\n",
    "\n",
    "注意：這份是「教育/專題」用途，不構成投資建議。\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import yfinance as yf\n",
    "\n",
    "# ========== 全域設定 ==========\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), \"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========== 下載資料 ==========\n",
    "def fetch_data(ticker: str, start_date: str, end_date: str):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", auto_adjust=False)\n",
    "    if data is None or data.empty:\n",
    "        raise ValueError(\"無法抓取數據，請檢查網路 / ticker / 日期範圍\")\n",
    "\n",
    "    data = data.ffill().dropna()\n",
    "    prices = np.asarray(data[\"Close\"].values, dtype=np.float64).flatten()\n",
    "    volumes = np.asarray(data[\"Volume\"].values, dtype=np.float64).flatten()\n",
    "    dates = pd.to_datetime(data.index).values\n",
    "\n",
    "    mask = ~np.isnan(prices) & ~np.isnan(volumes)\n",
    "    idx = np.where(mask)[0]\n",
    "    return prices[idx], volumes[idx], dates[idx]\n",
    "\n",
    "\n",
    "# ========== 技術指標 ==========\n",
    "def calculate_technical_indicators(prices, volumes, ma_window=20, rsi_window=14):\n",
    "    prices = np.asarray(prices, dtype=np.float64).flatten()\n",
    "    volumes = np.asarray(volumes, dtype=np.float64).flatten()\n",
    "\n",
    "    n = len(prices)\n",
    "    if n < max(ma_window, rsi_window) + 2:\n",
    "        raise ValueError(\"資料太短，無法計算 MA/RSI。請拉長日期區間。\")\n",
    "\n",
    "    # MA（簡單移動平均）\n",
    "    ma = np.convolve(prices, np.ones(ma_window) / ma_window, mode=\"valid\")\n",
    "    ma = np.concatenate([np.full(ma_window - 1, ma[0]), ma])\n",
    "\n",
    "    # RSI（簡化版）\n",
    "    diff = np.diff(prices, prepend=prices[0])\n",
    "    gain = np.where(diff > 0, diff, 0.0)\n",
    "    loss = np.where(diff < 0, -diff, 0.0)\n",
    "\n",
    "    avg_gain = np.convolve(gain, np.ones(rsi_window) / rsi_window, mode=\"valid\")\n",
    "    avg_loss = np.convolve(loss, np.ones(rsi_window) / rsi_window, mode=\"valid\")\n",
    "    avg_gain = np.concatenate([np.full(rsi_window - 1, avg_gain[0]), avg_gain])\n",
    "    avg_loss = np.concatenate([np.full(rsi_window - 1, avg_loss[0]), avg_loss])\n",
    "\n",
    "    rs = np.where(avg_loss > 1e-12, avg_gain / (avg_loss + 1e-12), 100.0)\n",
    "    rsi = 100.0 - (100.0 / (1.0 + rs))  # 0~100\n",
    "\n",
    "    # Volume normalize（全段最大值）\n",
    "    v_max = np.max(volumes) if np.max(volumes) > 0 else 1.0\n",
    "    volume_norm = volumes / v_max\n",
    "\n",
    "    return ma, rsi, volume_norm\n",
    "\n",
    "\n",
    "# ========== 環境（非 Gym 版，專題可用） ==========\n",
    "class StockTradingEnv:\n",
    "    \"\"\"\n",
    "    動作:\n",
    "      0 Hold\n",
    "      1 Buy  (買一半資金)\n",
    "      2 Sell (賣一半持股)\n",
    "\n",
    "    狀態:\n",
    "      sequence: (window_size, 4)  [price_norm, ma_norm, rsi_norm, volume_norm]\n",
    "      extra_state: (2,)           [position_ratio, cash_ratio]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prices,\n",
    "        volumes,\n",
    "        dates,\n",
    "        window_size=20,\n",
    "        initial_balance=100000.0,\n",
    "        trading_cost=0.001,\n",
    "        cooldown_steps=2,\n",
    "        ma_window=20,\n",
    "        rsi_window=14,\n",
    "    ):\n",
    "        self.prices = np.asarray(prices, dtype=np.float64).flatten()\n",
    "        self.volumes = np.asarray(volumes, dtype=np.float64).flatten()\n",
    "        self.dates = dates\n",
    "\n",
    "        self.window_size = int(window_size)\n",
    "        self.initial_balance = float(initial_balance)\n",
    "        self.trading_cost = float(trading_cost)\n",
    "        self.cooldown_steps = int(cooldown_steps)\n",
    "\n",
    "        self.ma, self.rsi, self.volume = calculate_technical_indicators(\n",
    "            self.prices, self.volumes, ma_window=ma_window, rsi_window=rsi_window\n",
    "        )\n",
    "\n",
    "        self.n = len(self.prices)\n",
    "        self.max_steps = self.n - self.window_size - 2  # 保留 next_price\n",
    "        if self.max_steps <= 1:\n",
    "            raise ValueError(\"資料長度不足以跑環境，請增加資料。\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares = 0.0\n",
    "\n",
    "        self.trade_cooldown = 0\n",
    "        self.last_buy_price = None\n",
    "        self.last_buy_step = None\n",
    "\n",
    "        self.action_log = []\n",
    "        self.hold_streak = 0\n",
    "\n",
    "        self.prev_portfolio_value = self.initial_balance  # 用於增量報酬\n",
    "        return self._get_state()\n",
    "\n",
    "    def _portfolio_value(self, price):\n",
    "        return float(self.balance + self.shares * price)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        # 推進一步\n",
    "        self.current_step += 1\n",
    "        idx = self.current_step + self.window_size - 1\n",
    "\n",
    "        current_price = float(self.prices[idx])\n",
    "        current_ma = float(self.ma[idx])\n",
    "        current_rsi = float(self.rsi[idx])  # 0~100\n",
    "        current_date = self.dates[idx]\n",
    "\n",
    "        # 下一天價格用於增量 PV（避免 look-ahead 的話，你也可以改成用當天 PV 差）\n",
    "        next_price = float(self.prices[idx + 1])\n",
    "\n",
    "        reward = 0.0\n",
    "        info_trade_profit = 0.0\n",
    "        sold_shares = 0.0\n",
    "\n",
    "        # cooldown\n",
    "        self.trade_cooldown = max(0, self.trade_cooldown - 1)\n",
    "\n",
    "        # --------- 動作處理 ----------\n",
    "        if action == 1 and self.trade_cooldown == 0:\n",
    "            # Buy: 用一半現金買入\n",
    "            if self.balance >= current_price:\n",
    "                budget = self.balance * 0.5\n",
    "                shares_to_buy = int(budget // current_price)\n",
    "                if shares_to_buy > 0:\n",
    "                    cost = shares_to_buy * current_price * (1.0 + self.trading_cost)\n",
    "                    if self.balance >= cost:\n",
    "                        self.balance -= cost\n",
    "                        self.shares += shares_to_buy\n",
    "                        self.last_buy_price = current_price\n",
    "                        self.last_buy_step = self.current_step\n",
    "                        self.action_log.append(f\"Buy {shares_to_buy} @ {current_price:.2f}\")\n",
    "                        self.hold_streak = 0\n",
    "                        self.trade_cooldown = self.cooldown_steps\n",
    "\n",
    "                        # 低買導向 shaping：低於 MA、RSI 偏低 -> 更高獎勵\n",
    "                        if current_price < current_ma:\n",
    "                            reward += 0.5\n",
    "                        if current_rsi < 35:\n",
    "                            reward += 0.5\n",
    "                    else:\n",
    "                        self.action_log.append(\"Failed Buy (cost)\")\n",
    "                        reward -= 0.2\n",
    "                else:\n",
    "                    self.action_log.append(\"Buy 0 (budget too small)\")\n",
    "                    reward -= 0.05\n",
    "            else:\n",
    "                self.action_log.append(\"Failed Buy (insufficient cash)\")\n",
    "                reward -= 0.2\n",
    "\n",
    "        elif action == 2 and self.shares > 0 and self.trade_cooldown == 0:\n",
    "            # Sell: 賣一半持股\n",
    "            sell_qty = int(self.shares * 0.5)\n",
    "            sell_qty = max(1, sell_qty)  # 至少賣 1 股（若 shares>0）\n",
    "            sell_qty = min(sell_qty, int(self.shares))\n",
    "\n",
    "            revenue = sell_qty * current_price * (1.0 - self.trading_cost)\n",
    "            self.balance += revenue\n",
    "            self.shares -= sell_qty\n",
    "\n",
    "            sold_shares = float(sell_qty)\n",
    "            self.action_log.append(f\"Sell {sell_qty} @ {current_price:.2f}\")\n",
    "            self.hold_streak = 0\n",
    "            self.trade_cooldown = self.cooldown_steps\n",
    "\n",
    "            # 低買高賣：若有 last_buy_price，計算已實現獲利\n",
    "            if self.last_buy_price is not None:\n",
    "                info_trade_profit = (current_price - self.last_buy_price) * sold_shares\n",
    "\n",
    "                # 高賣導向 shaping：高於 MA、RSI 偏高 -> 更高獎勵\n",
    "                if current_price > current_ma:\n",
    "                    reward += 0.5\n",
    "                if current_rsi > 65:\n",
    "                    reward += 0.5\n",
    "\n",
    "                # 真正「賺錢」才給大獎勵，「虧錢」給懲罰\n",
    "                if info_trade_profit > 0:\n",
    "                    reward += 1.0 + (info_trade_profit / (self.initial_balance + 1e-12)) * 200.0\n",
    "                else:\n",
    "                    reward -= 0.5 + (abs(info_trade_profit) / (self.initial_balance + 1e-12)) * 50.0\n",
    "\n",
    "            # 如果賣到剩很少（或清倉），可選擇重置 last_buy_price\n",
    "            if self.shares <= 0:\n",
    "                self.shares = 0.0\n",
    "                self.last_buy_price = None\n",
    "                self.last_buy_step = None\n",
    "\n",
    "        else:\n",
    "            # Hold\n",
    "            self.action_log.append(\"Hold\")\n",
    "            self.hold_streak += 1\n",
    "            # 太久不動給小懲罰（避免永遠 Hold）\n",
    "            reward += 0.05\n",
    "            if self.hold_streak >= 10:\n",
    "                reward -= 0.05\n",
    "\n",
    "        # --------- 增量資產報酬（核心 reward） ----------\n",
    "        pv_now = self._portfolio_value(current_price)\n",
    "        pv_next = self._portfolio_value(next_price)\n",
    "\n",
    "        # 用「下一步 PV - 本步 PV」做主要獎勵，避免只靠 shaping\n",
    "        delta = (pv_next - pv_now) / (pv_now + 1e-12)\n",
    "        reward += delta * 500.0\n",
    "\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        # episode 結束時加總回報（讓模型更重視長期）\n",
    "        if done:\n",
    "            total_return = (pv_now - self.initial_balance) / (self.initial_balance + 1e-12)\n",
    "            reward += total_return * 300.0\n",
    "\n",
    "        info = {\n",
    "            \"date\": current_date,\n",
    "            \"action\": self.action_log[-1],\n",
    "            \"price\": current_price,\n",
    "            \"shares\": float(self.shares),\n",
    "            \"balance\": float(self.balance),\n",
    "            \"portfolio_value\": float(pv_now),\n",
    "            \"sold_shares\": float(sold_shares),\n",
    "            \"trade_profit\": float(info_trade_profit),\n",
    "        }\n",
    "        return self._get_state(), float(reward), bool(done), info\n",
    "\n",
    "    def _get_state(self):\n",
    "        s = self.current_step\n",
    "        e = s + self.window_size\n",
    "\n",
    "        price_window = self.prices[s:e].astype(np.float64)\n",
    "        ma_window = self.ma[s:e].astype(np.float64)\n",
    "        rsi_window = self.rsi[s:e].astype(np.float64) / 100.0  # 0~1\n",
    "        volume_window = self.volume[s:e].astype(np.float64)     # 0~1\n",
    "\n",
    "        # 正規化：用視窗末端價格作基準，避免用全域 max（降低資料洩漏與尺度不穩）\n",
    "        ref_price = price_window[-1] if price_window[-1] > 0 else 1.0\n",
    "        price_norm = price_window / ref_price\n",
    "        ma_norm = ma_window / ref_price\n",
    "\n",
    "        sequence = np.stack([price_norm, ma_norm, rsi_window, volume_window], axis=1).astype(np.float32)\n",
    "\n",
    "        # 額外狀態：持股/現金比例（簡化）\n",
    "        # position_ratio：用「持股市值 / 初始資金」近似\n",
    "        current_price = ref_price\n",
    "        position_value = self.shares * current_price\n",
    "        position_ratio = np.clip(position_value / (self.initial_balance + 1e-12), 0.0, 10.0)\n",
    "\n",
    "        cash_ratio = np.clip(self.balance / (self.initial_balance + 1e-12), 0.0, 10.0)\n",
    "        extra_state = np.array([position_ratio, cash_ratio], dtype=np.float32)\n",
    "\n",
    "        return sequence, extra_state\n",
    "\n",
    "\n",
    "# ========== LSTM + Dueling DQN ==========\n",
    "class LSTM_DuelingDQN(nn.Module):\n",
    "    def __init__(self, feature_size: int, action_size: int, lstm_hidden=64, lstm_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_size,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if lstm_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        combined_size = lstm_hidden + 2  # + extra_state(2)\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(combined_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence, extra_state):\n",
    "        # sequence: (B, T, F)\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_feat = lstm_out[:, -1, :]  # (B, H)\n",
    "\n",
    "        combined = torch.cat([lstm_feat, extra_state], dim=1)  # (B, H+2)\n",
    "        x = self.feature(combined)\n",
    "\n",
    "        v = self.value(x)                 # (B, 1)\n",
    "        a = self.advantage(x)             # (B, A)\n",
    "        q = v + a - a.mean(dim=1, keepdim=True)\n",
    "        return q\n",
    "\n",
    "\n",
    "# ========== DQN Agent（Double DQN + 目標網路） ==========\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size=3,\n",
    "        feature_size=4,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        memory_size=50000,\n",
    "        batch_size=64,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_min=0.05,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_steps=500,\n",
    "        temperature=1.0,  # 可選：Boltzmann\n",
    "        use_boltzmann=False,\n",
    "    ):\n",
    "        self.action_size = action_size\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.gamma = float(gamma)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.epsilon = float(epsilon_start)\n",
    "        self.epsilon_min = float(epsilon_min)\n",
    "        self.epsilon_decay = float(epsilon_decay)\n",
    "\n",
    "        self.temperature = float(temperature)\n",
    "        self.use_boltzmann = bool(use_boltzmann)\n",
    "\n",
    "        self.memory = deque(maxlen=int(memory_size))\n",
    "\n",
    "        self.model = LSTM_DuelingDQN(feature_size=feature_size, action_size=action_size).to(DEVICE)\n",
    "        self.target_model = LSTM_DuelingDQN(feature_size=feature_size, action_size=action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=float(lr))\n",
    "        self.criterion = nn.SmoothL1Loss()  # Huber loss\n",
    "\n",
    "        self.learn_step = 0\n",
    "        self.target_update_steps = int(target_update_steps)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, int(action), float(reward), next_state, float(done)))\n",
    "\n",
    "    def act(self, state):\n",
    "        sequence, extra_state = state\n",
    "\n",
    "        # epsilon 探索：直接隨機\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        seq_t = torch.tensor(sequence, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        extra_t = torch.tensor(extra_state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = self.model(seq_t, extra_t)  # (1, A)\n",
    "\n",
    "        if self.use_boltzmann:\n",
    "            prob = torch.softmax(q / max(self.temperature, 1e-6), dim=1).detach().cpu().numpy()[0]\n",
    "            return int(np.random.choice(self.action_size, p=prob))\n",
    "\n",
    "        return int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        sequences = torch.tensor([t[0][0] for t in minibatch], dtype=torch.float32, device=DEVICE)\n",
    "        extra_states = torch.tensor([t[0][1] for t in minibatch], dtype=torch.float32, device=DEVICE)\n",
    "        actions = torch.tensor([t[1] for t in minibatch], dtype=torch.long, device=DEVICE)\n",
    "        rewards = torch.tensor([t[2] for t in minibatch], dtype=torch.float32, device=DEVICE)\n",
    "        next_sequences = torch.tensor([t[3][0] for t in minibatch], dtype=torch.float32, device=DEVICE)\n",
    "        next_extra_states = torch.tensor([t[3][1] for t in minibatch], dtype=torch.float32, device=DEVICE)\n",
    "        dones = torch.tensor([t[4] for t in minibatch], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "        # Q(s,a)\n",
    "        q_values = self.model(sequences, extra_states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Double DQN：\n",
    "        # online 網路選下一步 action，target 網路估值\n",
    "        with torch.no_grad():\n",
    "            next_actions = torch.argmax(self.model(next_sequences, next_extra_states), dim=1)\n",
    "            next_q = self.target_model(next_sequences, next_extra_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q = rewards + (1.0 - dones) * self.gamma * next_q\n",
    "\n",
    "        loss = self.criterion(q_values, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        # target update\n",
    "        self.learn_step += 1\n",
    "        if self.learn_step % self.target_update_steps == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # temperature（如果用 boltzmann）\n",
    "        if self.use_boltzmann:\n",
    "            self.temperature = max(0.1, self.temperature * 0.999)\n",
    "\n",
    "        return float(loss.item())\n",
    "\n",
    "    def save(self, path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.model.state_dict(),\n",
    "                \"target\": self.target_model.state_dict(),\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"learn_step\": self.learn_step,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        ckpt = torch.load(path, map_location=DEVICE)\n",
    "        self.model.load_state_dict(ckpt[\"model\"])\n",
    "        self.target_model.load_state_dict(ckpt[\"target\"])\n",
    "        self.epsilon = float(ckpt.get(\"epsilon\", 0.0))\n",
    "        self.learn_step = int(ckpt.get(\"learn_step\", 0))\n",
    "\n",
    "\n",
    "# ========== 訓練 + 測試 ==========\n",
    "def train_and_test_dqn(\n",
    "    ticker=\"SPMO\",\n",
    "    end_date=datetime(2024, 5, 9),\n",
    "    train_years=8,\n",
    "    test_years=1,\n",
    "    window_size=20,\n",
    "    initial_balance=100000.0,\n",
    "    episodes=200,\n",
    "):\n",
    "    end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    train_start = (end_date - timedelta(days=int(train_years * 365))).strftime(\"%Y-%m-%d\")\n",
    "    test_start = (end_date - timedelta(days=int(test_years * 365))).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    prices, volumes, dates = fetch_data(ticker, train_start, end_str)\n",
    "\n",
    "    train_mask = dates < np.datetime64(test_start)\n",
    "    test_mask = ~train_mask\n",
    "\n",
    "    train_env = StockTradingEnv(\n",
    "        prices[train_mask], volumes[train_mask], dates[train_mask],\n",
    "        window_size=window_size, initial_balance=initial_balance, trading_cost=0.001\n",
    "    )\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        action_size=3,\n",
    "        feature_size=4,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        memory_size=50000,\n",
    "        batch_size=64,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_min=0.05,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_steps=500,\n",
    "        use_boltzmann=False,\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(OUTPUT_DIR, \"lstm_dueling_double_dqn_best.pth\")\n",
    "    best_pv = -1e18\n",
    "\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Train range: {train_start} ~ {test_start} (exclusive)\")\n",
    "    print(f\"Test  range: {test_start} ~ {end_str}\")\n",
    "    print(f\"Save model to: {model_path}\")\n",
    "\n",
    "    for ep in range(1, episodes + 1):\n",
    "        state = train_env.reset()\n",
    "        total_reward = 0.0\n",
    "        losses = []\n",
    "        action_cnt = [0, 0, 0]\n",
    "\n",
    "        for _ in range(train_env.max_steps):\n",
    "            action = agent.act(state)\n",
    "            action_cnt[action] += 1\n",
    "\n",
    "            next_state, reward, done, info = train_env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            loss = agent.replay()\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        pv = train_env.balance + train_env.shares * float(train_env.prices[-1])\n",
    "        avg_loss = float(np.mean(losses)) if losses else 0.0\n",
    "\n",
    "        if pv > best_pv:\n",
    "            best_pv = pv\n",
    "            agent.save(model_path)\n",
    "\n",
    "        if ep % 10 == 0 or ep == 1:\n",
    "            print(\n",
    "                f\"Ep {ep:4d}/{episodes} | Reward {total_reward:8.2f} | PV {pv:10.2f} | \"\n",
    "                f\"Loss {avg_loss:8.4f} | eps {agent.epsilon:5.3f} | \"\n",
    "                f\"Cnt(H/B/S)={action_cnt}\"\n",
    "            )\n",
    "            print(\"Recent actions:\", train_env.action_log[-8:])\n",
    "\n",
    "    # ========== 測試 ==========\n",
    "    test_env = StockTradingEnv(\n",
    "        prices[test_mask], volumes[test_mask], dates[test_mask],\n",
    "        window_size=window_size, initial_balance=initial_balance, trading_cost=0.001\n",
    "    )\n",
    "\n",
    "    agent.load(model_path)\n",
    "    agent.epsilon = 0.0  # 測試不探索\n",
    "\n",
    "    state = test_env.reset()\n",
    "    results = []\n",
    "\n",
    "    for _ in range(test_env.max_steps):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = test_env.step(action)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Date\": info[\"date\"],\n",
    "                \"Action\": info[\"action\"],\n",
    "                \"Price\": info[\"price\"],\n",
    "                \"Shares\": info[\"shares\"],\n",
    "                \"Balance\": info[\"balance\"],\n",
    "                \"Portfolio_Value\": info[\"portfolio_value\"],\n",
    "                \"Trade_Profit\": info[\"trade_profit\"],\n",
    "            }\n",
    "        )\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    out_csv = os.path.join(OUTPUT_DIR, f\"{ticker}_lstm_dqn_test_results.csv\")\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    final_pv = float(results[-1][\"Portfolio_Value\"]) if results else float(\"nan\")\n",
    "    hold_cnt = sum(r[\"Action\"] == \"Hold\" for r in results)\n",
    "    buy_cnt = sum(str(r[\"Action\"]).startswith(\"Buy\") for r in results)\n",
    "    sell_cnt = sum(str(r[\"Action\"]).startswith(\"Sell\") for r in results)\n",
    "\n",
    "    print(\"\\n==== 測試完成 ====\")\n",
    "    print(f\"測試結果已儲存：{out_csv}\")\n",
    "    print(f\"最終資產：{final_pv:.2f}\")\n",
    "    print(f\"動作統計：Hold={hold_cnt}, Buy={buy_cnt}, Sell={sell_cnt}\")\n",
    "    print(f\"Best PV in training：{best_pv:.2f}\")\n",
    "\n",
    "    return model_path, out_csv\n",
    "\n",
    "\n",
    "def predict_today_action(\n",
    "    model_path,\n",
    "    ticker=\"SPMO\",\n",
    "    window_size=20,\n",
    "    initial_balance=100000.0,\n",
    "    lookback_days=120,\n",
    "):\n",
    "    agent = DQNAgent(action_size=3, feature_size=4)\n",
    "    agent.load(model_path)\n",
    "    agent.epsilon = 0.0\n",
    "\n",
    "    today = datetime.today()\n",
    "    start_date = (today - timedelta(days=int(lookback_days))).strftime(\"%Y-%m-%d\")\n",
    "    end_date = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    prices, volumes, dates = fetch_data(ticker, start_date, end_date)\n",
    "    env = StockTradingEnv(\n",
    "        prices, volumes, dates,\n",
    "        window_size=window_size, initial_balance=initial_balance, trading_cost=0.001\n",
    "    )\n",
    "    state = env.reset()\n",
    "\n",
    "    info = None\n",
    "    action = 0\n",
    "    for _ in range(env.max_steps):\n",
    "        action = agent.act(state)\n",
    "        state, _, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    action_str = [\"Hold\", \"Buy\", \"Sell\"][action]\n",
    "    print(f\"\\n[{pd.to_datetime(info['date']).date()}] 建議動作：{action_str}\")\n",
    "    print(\n",
    "        f\"價格：{info['price']:.2f} | 持股：{info['shares']:.0f} | 現金：{info['balance']:.2f} | 資產：{info['portfolio_value']:.2f}\"\n",
    "    )\n",
    "\n",
    "    # 存檔紀錄\n",
    "    daily_log = os.path.join(OUTPUT_DIR, \"daily_signal_log.csv\")\n",
    "    row = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Date\": info[\"date\"],\n",
    "                \"Action\": action_str,\n",
    "                \"Price\": info[\"price\"],\n",
    "                \"Shares\": info[\"shares\"],\n",
    "                \"Balance\": info[\"balance\"],\n",
    "                \"Portfolio_Value\": info[\"portfolio_value\"],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    if os.path.exists(daily_log):\n",
    "        row.to_csv(daily_log, mode=\"a\", header=False, index=False, encoding=\"utf-8-sig\")\n",
    "    else:\n",
    "        row.to_csv(daily_log, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"今日訊號已紀錄：{daily_log}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 你可以改 ticker：\n",
    "    # - 若你真的是要「0050」：yfinance 通常是 \"0050.TW\"\n",
    "    # - 你原碼是 \"SPMO\"：我保留但你可自行改\n",
    "    ticker = \"SPMO\"   # 例如改成 \"0050.TW\"\n",
    "\n",
    "    best_model_path, test_csv_path = train_and_test_dqn(\n",
    "        ticker=ticker,\n",
    "        end_date=datetime(2024, 5, 9),\n",
    "        train_years=8,\n",
    "        test_years=1,\n",
    "        window_size=20,\n",
    "        initial_balance=100000.0,\n",
    "        episodes=200,\n",
    "    )\n",
    "\n",
    "    predict_today_action(\n",
    "        model_path=best_model_path,\n",
    "        ticker=ticker,\n",
    "        window_size=20,\n",
    "        initial_balance=100000.0,\n",
    "        lookback_days=180,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
